{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T20:02:37.257178Z",
     "start_time": "2018-03-24T20:02:36.089631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemnaniv/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T20:20:16.522340Z",
     "start_time": "2018-03-24T20:20:16.162001Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(\n",
    "        self, train_X, train_Y, test_X, test_Y, \n",
    "        n_input=None, n_output=None, learning_rate=0.001, n_iterations=1000, beta=0.0001,\n",
    "        n_hidden_layers=2, n_nodes_per_layer=[256, 256], batch_size=128, display_step=100,\n",
    "        activation=tf.nn.leaky_relu, output_activation=tf.nn.tanh\n",
    "    ):       \n",
    "        self.trainX = train_X\n",
    "        self.trainY = train_Y\n",
    "        self.testX = test_X\n",
    "        self.testY = test_Y\n",
    "        \n",
    "        self.n_instances = self.trainX.shape[0]\n",
    "        self.n_features = self.trainX.shape[1]\n",
    "        self.n_classes = self.trainY.shape[1]\n",
    "\n",
    "        # setting initial parameters\n",
    "        self.n_input = n_input if n_input is not None else self.n_features\n",
    "        self.n_output = n_output if n_output is not None else self.n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_layers = n_hidden_layers\n",
    "        # self.n_nodes_per_layer = n_nodes_per_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.display_step = display_step\n",
    "        \n",
    "        self.beta = beta  # parameter for L2 loss\n",
    "        \n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        \n",
    "        # compute initial weights and biases for all layers\n",
    "        self.wt, self.b = self._generate_initial_weights(n_nodes_per_layer)\n",
    "        \n",
    "        # graph placeholders\n",
    "        self.X = tf.placeholder(\"float\", [None, self.n_input])\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.n_output])\n",
    "        \n",
    "        # perceptron and output perceptron computation\n",
    "        self.perceptron = lambda h, w, b: activation(tf.add(tf.matmul(h, w), b))\n",
    "        self.output_perceptron = lambda h, w, b: output_activation(tf.add(tf.matmul(h, w), b))\n",
    "        \n",
    "    \n",
    "    def _generate_initial_weights(self, n_nodes_per_layer):\n",
    "        weights = dict()\n",
    "        biases = dict()\n",
    "        prev_layer_n_output = self.n_input\n",
    "        for i in range(self.n_layers):\n",
    "            weights.update({\n",
    "                'layer_{}'.format(i+1): tf.Variable(tf.random_normal([prev_layer_n_output, n_nodes_per_layer[i]]))\n",
    "            })\n",
    "            biases.update({\n",
    "                'layer_{}'.format(i+1): tf.Variable(tf.random_normal([n_nodes_per_layer[i]]))\n",
    "            })\n",
    "            prev_layer_n_output = n_nodes_per_layer[i]\n",
    "        # add the weight and bias for output layer\n",
    "        weights.update({\n",
    "            'output': tf.Variable(tf.random_normal([prev_layer_n_output, self.n_output]))\n",
    "        })\n",
    "        biases.update({\n",
    "            'output': tf.Variable(tf.random_normal([self.n_output]))\n",
    "        })\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    def next_batch(self, X, Y):\n",
    "        \"\"\"Return the next `batch_size`. Generalizes TF's internal implementation\"\"\"\n",
    "        _x = X\n",
    "        _y = Y\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += self.batch_size\n",
    "        if self._index_in_epoch > self.n_instances:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.n_instances)\n",
    "            np.random.shuffle(perm)\n",
    "            _x = _x[perm]\n",
    "            _y = _y[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = self.batch_size\n",
    "            assert self.batch_size <= self.n_instances\n",
    "        end = self._index_in_epoch\n",
    "        return _x[start:end], _y[start:end]\n",
    "    \n",
    "    def neural_net(self, x):\n",
    "        _p = self.perceptron\n",
    "        _op = self.output_perceptron\n",
    "        \n",
    "        layer_outputs = []\n",
    "        prev_layer_output = x\n",
    "        for i in range(self.n_layers):\n",
    "            if len(layer_outputs) == 0:\n",
    "                _input = x\n",
    "            else:\n",
    "                _input = layer_outputs[-1]\n",
    "            layer_outputs.append(_p(_input, self.wt['layer_{}'.format(i+1)], self.b['layer_{}'.format(i+1)]))\n",
    "        output = _op(layer_outputs[-1], self.wt['output'], self.b['output'])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self):\n",
    "        predictions = self.neural_net(self.X)\n",
    "        \n",
    "        regularizer_additive_value = np.sum([self.beta*tf.nn.l2_loss(arr) for _, arr in self.wt.items()]) + np.sum([self.beta*tf.nn.l2_loss(arr) for _, arr in self.b.items()])\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=self.Y)# + regularizer_additive_value\n",
    "        )\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss_op)\n",
    "\n",
    "        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(self.Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Initialize the variables (i.e. assign their default value) \n",
    "            # and Run the initializer\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for step in range(1, self.n_iterations+1):\n",
    "                batch_x, batch_y = self.next_batch(self.trainX, self.trainY)                  \n",
    "                \n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "                if step % self.display_step == 0 or step == 1:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "                    print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "\n",
    "            # Calculate accuracy for MNIST test images\n",
    "            print(\"Testing Accuracy:\", \\\n",
    "                  sess.run(accuracy, feed_dict={self.X: self.testX, self.Y: self.testY}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T16:25:10.151728Z",
     "start_time": "2018-03-19T16:25:06.381018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "mnist = mnist_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T16:25:11.679771Z",
     "start_time": "2018-03-19T16:25:11.676515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# _x, _y = mnist.train.next_batch(50)\n",
    "# print(_x.shape, _y.shape)\n",
    "# mnist.test.images.shape\n",
    "print(type(mnist.train.images), type(mnist.train.labels), type(mnist.test.images), type(mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T22:26:51.801184Z",
     "start_time": "2018-03-18T22:26:47.435608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1020.0073, Training Accuracy= 0.195\n",
      "Step 100, Minibatch Loss= 18.9039, Training Accuracy= 0.805\n",
      "Step 200, Minibatch Loss= 7.8815, Training Accuracy= 0.891\n",
      "Step 300, Minibatch Loss= 0.8989, Training Accuracy= 0.969\n",
      "Step 400, Minibatch Loss= 3.1837, Training Accuracy= 0.922\n",
      "Step 500, Minibatch Loss= 8.2724, Training Accuracy= 0.891\n",
      "Step 600, Minibatch Loss= 6.7082, Training Accuracy= 0.914\n",
      "Step 700, Minibatch Loss= 4.5275, Training Accuracy= 0.945\n",
      "Step 800, Minibatch Loss= 1.3480, Training Accuracy= 0.961\n",
      "Step 900, Minibatch Loss= 2.7980, Training Accuracy= 0.938\n",
      "Step 1000, Minibatch Loss= 1.0324, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "NN(mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels).optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 News Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T20:02:44.905415Z",
     "start_time": "2018-03-24T20:02:41.079703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import 20NG data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ngtrain = fetch_20newsgroups(subset='train', data_home='../Data')\n",
    "ngtest = fetch_20newsgroups(subset='test', data_home='../Data')\n",
    "ngtrain_vectorizer = TfidfVectorizer(stop_words='english', strip_accents='ascii')\n",
    "_ngtrain_vectors = ngtrain_vectorizer.fit_transform(ngtrain.data)\n",
    "ngtest_vectorizer = TfidfVectorizer(stop_words='english', strip_accents='ascii')\n",
    "_ngtest_vectors = ngtest_vectorizer.fit_transform(ngtest.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T20:02:47.589996Z",
     "start_time": "2018-03-24T20:02:45.213258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 49601) (11314, 20) (7532, 49601) (7532, 20)\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> <class 'numpy.ndarray'> <class 'numpy.matrixlib.defmatrix.matrix'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# remove features in test that are not found in train\n",
    "one_hot_encode = lambda n_classes, target: np.eye(n_classes)[target]\n",
    "get_indices = lambda x, y: list(set(np.searchsorted(x, y)))\n",
    "\n",
    "common_features = np.intersect1d(ngtrain_vectorizer.get_feature_names(), ngtest_vectorizer.get_feature_names())\n",
    "ngtrain_X = _ngtrain_vectors[:, get_indices(ngtrain_vectorizer.get_feature_names(), common_features)].todense()\n",
    "ngtest_X = _ngtest_vectors[:, get_indices(ngtest_vectorizer.get_feature_names(), common_features)].todense()\n",
    "ngtrain_Y = one_hot_encode(20, ngtrain.target)\n",
    "ngtest_Y = one_hot_encode(20, ngtest.target)\n",
    "\n",
    "print(ngtrain_X.shape, ngtrain_Y.shape, ngtest_X.shape, ngtest_Y.shape)\n",
    "print(type(ngtrain_X), type(ngtrain_Y), type(ngtest_X), type(ngtest_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T22:27:54.618477Z",
     "start_time": "2018-03-18T22:27:14.128991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 130.7787, Training Accuracy= 0.141\n",
      "Step 100, Minibatch Loss= 1.0607, Training Accuracy= 0.883\n",
      "Step 200, Minibatch Loss= 0.0447, Training Accuracy= 0.984\n",
      "Step 300, Minibatch Loss= 0.0977, Training Accuracy= 0.992\n",
      "Step 400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "Step 500, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "Step 600, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "Step 700, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "Step 800, Minibatch Loss= 0.0584, Training Accuracy= 0.992\n",
      "Step 900, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "Step 1000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.58032393\n"
     ]
    }
   ],
   "source": [
    "NN(ngtrain_X, ngtrain_Y, ngtest_X, ngtest_Y,\n",
    "   #learning_rate=0.01, n_iterations=500, n_hidden_layers=2, n_nodes_per_layer=[256, 256],\n",
    "   #batch_size=128, display_step=100, activation=tf.nn.leaky_relu\n",
    "  ).optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T20:22:16.140487Z",
     "start_time": "2018-03-24T20:21:20.253552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 557.9946, Training Accuracy= 0.039\n",
      "Step 100, Minibatch Loss= 47.9567, Training Accuracy= 0.180\n",
      "Step 200, Minibatch Loss= 27.1920, Training Accuracy= 0.375\n",
      "Step 300, Minibatch Loss= 18.8638, Training Accuracy= 0.531\n",
      "Step 400, Minibatch Loss= 11.7341, Training Accuracy= 0.641\n",
      "Step 500, Minibatch Loss= 5.4132, Training Accuracy= 0.781\n",
      "Step 600, Minibatch Loss= 3.9951, Training Accuracy= 0.867\n",
      "Step 700, Minibatch Loss= 1.7166, Training Accuracy= 0.914\n",
      "Step 800, Minibatch Loss= 1.0871, Training Accuracy= 0.969\n",
      "Step 900, Minibatch Loss= 1.2516, Training Accuracy= 0.938\n",
      "Step 1000, Minibatch Loss= 0.8755, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.40228358\n"
     ]
    }
   ],
   "source": [
    "NN(ngtrain_X, ngtrain_Y, ngtest_X, ngtest_Y,\n",
    "    n_hidden_layers=2, n_nodes_per_layer=[512, 512], output_activation=tf.nn.leaky_relu\n",
    "  ).optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
